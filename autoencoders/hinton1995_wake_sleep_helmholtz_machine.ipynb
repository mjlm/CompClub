{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook implements the Helmholtz machine introduced in:\n",
    "\n",
    "Hinton GE, Dayan P, Frey BJ & Neal RM (1995). The “wake-sleep” algorithm for unsupervised neural networks. Science 268, 1158–1161. ([PDF](http://www.gatsby.ucl.ac.uk/~dayan/papers/hdfn95.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up\n",
    "To organize the code, we'll first create a class representing one layer of our Helmholtz machine.\n",
    "\n",
    "In this class, each layer is described by its neighbors (the layer above and the layer below) and the weights originating from the layer (i.e. there will be two nUnitsInLayer-by-something weight matrices per layer, one for the recognition weights and one for the generative weights).\n",
    "\n",
    "The nomenclature follows Figure 1 in Hinton 1995: The input layer is at the lowest positon. Recognition weights point upwards, and generative weights point downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%qtconsole\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n_units, layer_below=None):\n",
    "        self.n_units = n_units\n",
    "        self.state = np.zeros(n_units)\n",
    "        self.b_rec = np.zeros(self.n_units) # Recognition bias\n",
    "        self.b_gen = np.zeros(self.n_units) # Generative bias\n",
    "        self.above = None # Layer above; will be filled when connecting layer from above.\n",
    "        \n",
    "        if layer_below is not None:            \n",
    "            # Create link from self to layer below and initialize\n",
    "            # the down-going (generative) weights:\n",
    "            self.below = layer_below\n",
    "            self.init_w_gen()\n",
    "                        \n",
    "            # Create link from layer below to self and initialize\n",
    "            # the up-going (recognition) weights:\n",
    "            layer_below.above = self\n",
    "            layer_below.init_w_rec()\n",
    "        else:\n",
    "            self.below = None\n",
    "            \n",
    "    def init_w_rec(self):\n",
    "        \"\"\"Initialize reconstruction weights originating from this layer.\"\"\"\n",
    "        self.w_rec = np.zeros((self.n_units, self.above.n_units))\n",
    "        \n",
    "    def init_w_gen(self):\n",
    "        \"\"\"Initialize generative weights originating from this layer.\"\"\"\n",
    "        self.w_gen = np.zeros((self.n_units, self.below.n_units))\n",
    "        \n",
    "    def get_C(self):\n",
    "        \"\"\"Get the description length of all units in the layer.\"\"\"\n",
    "        # Equations 2 and 3 in Hinton 1995.\n",
    "        \n",
    "        if self.above is None:\n",
    "            p = sigmoid(-self.b_gen)\n",
    "        else:\n",
    "            p = sigmoid(-self.b_gen - self.above.state.dot(self.above.w_gen).T)\n",
    "        return np.sum(-(self.state * np.log(p)) - (1-self.state) * np.log(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wake phase\n",
    "\n",
    "In the wake phase, the recognition weights are fixed and the generative weights are trained.\n",
    "\n",
    "Equation 4 in Hinton 1995 provides the update rule:\n",
    "\n",
    "$$ \\Delta w_{kj} = \\epsilon s_k^\\alpha (s_j^\\alpha - p_j^\\alpha) $$\n",
    "\n",
    "This is the derivative of the objective given in equation (3). The update specifies that for each active presynaptic neuron, the weight of the connection to each postsynaptic neuron should change in proportion to the difference between the current state of the postsynaptic neuron and the probability that the neuron should be active, as specified by the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(x))\n",
    "\n",
    "def drive_layer_wake(layer_below, layer_above):\n",
    "    \"\"\"Determine neuronal activations according to the recognition weights.\"\"\"\n",
    "    # In the wake phase, we use the recognition weights to choose activations,\n",
    "    # from the input layer at the bottom upwards.\n",
    "    \n",
    "    # Equation 1 in Hinton 1995:\n",
    "    p = sigmoid(-layer_above.b_rec - layer_below.state.dot(layer_below.w_rec).T)\n",
    "    \n",
    "    # Sample binary states according from distribution p:\n",
    "    layer_above.state = p > np.random.random_sample(p.shape)\n",
    "\n",
    "def train_layer_wake(layer_below, layer_above):\n",
    "    \"\"\"Change generative weights based on recognition activations.\"\"\"\n",
    "    # Based on equation 4 in Hinton 1995.\n",
    "    \n",
    "    if layer_above is None:\n",
    "        # No layer_above means we're training the top layer:\n",
    "        # Just adjust the biases to match the current state.\n",
    "        p = sigmoid(-layer_below.b_gen)\n",
    "        layer_below.b_gen = layer_below.b_gen + (epsilon *\n",
    "                            (layer_below.state - p))\n",
    "    else:\n",
    "        p = sigmoid(-layer_below.b_gen - layer_above.state.dot(layer_above.w_gen).T)\n",
    "        layer_above.w_gen = layer_above.w_gen + (epsilon * \n",
    "                            np.outer(layer_above.state, (layer_below.state - p)))\n",
    "        # Update bias. This is not described in the paper, but\n",
    "        # http://www.gatsby.ucl.ac.uk/~dayan/papers/d2000a.pdf\n",
    "        # suggests that it should be done like this:\n",
    "        layer_below.b_gen = layer_below.b_gen + (epsilon *\n",
    "                            (layer_below.state - p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep phase\n",
    "In the sleep phase, we drive the network using the generative weights to dream up \"fantasies\" of what the input to the network might look like according to the generative model. We then change the recognition weights to match the generative model as closely as possible: Given one of the fantasies as input, the recognition weights should be such that, when the network is driven by this input, it goes to a state that resembles the state used to dream up that fantasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drive_layer_sleep(layer_below, layer_above):\n",
    "    \"\"\"Determine neuronal activations according to the generative weights.\"\"\"\n",
    "    \n",
    "    # Equation 1 in Hinton 1995:\n",
    "    p = sigmoid(-layer_below.b_gen - layer_above.state.dot(layer_above.w_gen).T)\n",
    "    \n",
    "    # Sample binary states according from distribution p:\n",
    "    layer_below.state = p > np.random.random_sample(p.shape)\n",
    "\n",
    "def train_layer_sleep(layer_below, layer_above):\n",
    "    \"\"\"Change recognition weights based on generative activations.\"\"\"\n",
    "    # Based on equation 7 in Hinton 1995.\n",
    "    \n",
    "    if layer_below is None:\n",
    "        # No layer_below means we're training the bottom layer:\n",
    "        # Just adjust the biases to match the current input.\n",
    "        q = sigmoid(-layer_above.b_rec)        \n",
    "        layer_above.b_rec = layer_above.b_rec + (epsilon *\n",
    "                            (layer_above.state - q))\n",
    "    else:\n",
    "        q = sigmoid(-layer_above.b_rec - layer_below.state.dot(layer_below.w_rec).T)\n",
    "        layer_below.w_rec = layer_below.w_rec + (epsilon * \n",
    "                            np.outer(layer_below.state, (layer_above.state - q)))\n",
    "        layer_above.b_rec = layer_above.b_rec + (epsilon *\n",
    "                           (layer_above.state - q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the layers together\n",
    "To orchestrate the training, we create a Network class. The Network object will contain a number of layers an coordinate the driving and training sweeps through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, nInput):\n",
    "        self.layers = [Layer(nInput)]\n",
    "        self.nLayers = 1\n",
    "        \n",
    "    def addLayer(self, nUnits):\n",
    "        self.layers.append(Layer(nUnits, self.layers[-1]))\n",
    "        self.nLayers += 1\n",
    "        \n",
    "    def drive_wake(self, input_state):\n",
    "        self.layers[0].state = input_state\n",
    "        for i in range(self.nLayers-1):\n",
    "            drive_layer_wake(self.layers[i], self.layers[i+1])\n",
    "        \n",
    "    def train_wake(self):\n",
    "        train_layer_wake(self.layers[-1], None)\n",
    "        for i in range(self.nLayers-2, -1, -1):\n",
    "            train_layer_wake(self.layers[i], self.layers[i+1])\n",
    "\n",
    "    def drive_sleep(self):\n",
    "        p = sigmoid(-self.layers[-1].b_gen)\n",
    "        self.layers[-1].state = np.random.random_sample(self.layers[-1].state.shape) < p\n",
    "        for i in range(self.nLayers-2, -1, -1):\n",
    "            drive_layer_sleep(self.layers[i], self.layers[i+1])\n",
    "\n",
    "    def train_sleep(self):\n",
    "        train_layer_sleep(None, self.layers[0])\n",
    "        for i in range(self.nLayers-1):\n",
    "            train_layer_sleep(self.layers[i], self.layers[i+1])\n",
    "            \n",
    "    def get_C(self):\n",
    "        \"\"\"Get description length of whole network.\"\"\"\n",
    "        return np.sum([layer.get_C() for layer in self.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "Hinton 1995 uses a handwritten digit dataset \"from the CEDAR CDROM 1\" (endnote 7). This dataset is still available, if you're willing to part with $950 and wait for a CDROM to be mailed to you. I am not, so I'm using the freely available MNIST dataset instead. Hinton's images measure 8 by 8 pixels. Downsampling the 28 by 28 MNIST images to 8 by 8 makes them almost unrecognizable. 14 by 14 is a reasonable compromise between image quality and training duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................Done.\n"
     ]
    }
   ],
   "source": [
    "import load_mnist as load_mnist\n",
    "from scipy.misc import imresize\n",
    "import sys\n",
    "\n",
    "# Load data (only training since we're not actually testing anything):\n",
    "im_train_raw, label_train = load_mnist.load_mnist(dataset=\"training\", digits=np.arange(10), path=\"C:\\\\Users\\\\Matthias\\\\GitHub\\\\personal\\\\neuralNets\\\\mnist\")\n",
    "\n",
    "# Only use the easier first half of the data:\n",
    "im_train_raw = im_train_raw[0:30000, :, :]\n",
    "label_train = label_train[0:30000]\n",
    "\n",
    "# Downsample dataset:\n",
    "h = 14\n",
    "w = 14\n",
    "im_train = np.zeros((im_train_raw.shape[0], h, w))\n",
    "for i in range(im_train.shape[0]):\n",
    "    img = im_train_raw[i,:,:,]\n",
    "    \n",
    "    # Crop and center digit so there is no whitespace around it:\n",
    "    B = np.argwhere(img)\n",
    "    (ystart, xstart), (ystop, xstop) = B.min(0), B.max(0) + 1\n",
    "    cropped = img[ystart:ystop, xstart:xstop]\n",
    "    square = np.zeros((np.max((xstop-xstart, ystop-ystart))) + (0, 0))\n",
    "    x_ind = np.arange(cropped.shape[1])+(square.shape[1]-cropped.shape[1])/2\n",
    "    y_ind = np.arange(cropped.shape[0])+(square.shape[0]-cropped.shape[0])/2\n",
    "    square[y_ind.min():y_ind.max()+1, x_ind.min():x_ind.max()+1] = cropped\n",
    "    im_train[i,:,:,] = imresize(square, (h, w))\n",
    "    if i%1000 == 0:\n",
    "        sys.stdout.write('.')\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "# Binarize dataset:\n",
    "im_train = im_train > 10 # 10 looks best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few examples to make sure the preprocessing worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAD/CAYAAAB2MoomAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa6bfb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(10):\n",
    "    ind_digit_here = (label_train==i).nonzero()\n",
    "    ind = np.random.randint(ind_digit_here[0].size)\n",
    "    img_row = im_train[ind_digit_here[0][ind],:,:]\n",
    "    for j in range(1, 12):\n",
    "        ind = np.random.randint(ind_digit_here[0].size)\n",
    "        img = im_train[ind_digit_here[0][ind],:,:]\n",
    "        img_row = np.concatenate((img_row, np.zeros((img.shape[0], 2)), img), 1)\n",
    "    if i is 0:\n",
    "        img_grid = img_row\n",
    "    else:\n",
    "        img_grid = np.concatenate((img_grid, np.zeros((2, img_row.shape[1])), img_row), 0)\n",
    "\n",
    "plt.imshow(-img_grid, interpolation=\"nearest\", cmap=\"gray\");\n",
    "plt.gca().axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the paper, we can use the \"naive\" description length (see endnote 7) of the data as a benchmark for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean description length for the current dataset is 91.0.\n"
     ]
    }
   ],
   "source": [
    "mean_desc_length_digit = np.zeros(10)\n",
    "for i in range(10):\n",
    "    i_digit_here = (label_train==i).nonzero()[0]\n",
    "    p = np.maximum(np.mean((im_train[i_digit_here, :, :]), 0).ravel(), 1e-10)\n",
    "    for i_example in range(i_digit_here.size):\n",
    "        state = im_train[i_digit_here[i_example], :, :].ravel()\n",
    "        mean_desc_length_digit[i] += np.sum(-(state * np.log(p)) - (1-state) * np.log(1-p))\n",
    "    mean_desc_length_digit[i] /= i_digit_here.size\n",
    "mean_desc_length = np.mean(mean_desc_length_digit)\n",
    "print('The mean description length for the current dataset is {:1.1f}.').format(mean_desc_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "\n",
    "I decided to skip the individual networks for the 10 digits and just build one large network which encodes all 10 digits. The description length provides a measure for the performance of the network.\n",
    "\n",
    "Hinton's input images measured 8 by 8 pixels and he used a 64-32-32-16 architecture for the single network that processes all digits jointly.\n",
    "\n",
    "Playing around with the network architecture didn't reveal any simple relationships between performance and architecture. Larger networks just took longer to train. So I'll just use Hinton's 32-32-16 hidden layer sizes. The input layer will have 196 units to match the image size (14*14).\n",
    "\n",
    "The weights are automatically initialized to zero by the Layer class defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input layer:\n",
    "nPix = h * w\n",
    "network = Network(nPix)\n",
    "\n",
    "# Add hidden layers:\n",
    "network.addLayer(32)\n",
    "network.addLayer(32)\n",
    "network.addLayer(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot description length to monitor training progress:\n",
    "%matplotlib auto\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "h_line, = plt.plot([], [], 'r-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Description length')\n",
    "xdata=[0]\n",
    "ydata=[0]\n",
    "\n",
    "# Learning rate:\n",
    "epsilon = 0.01 # Hinton 1995 uses 0.01 (see endnote 7).\n",
    "\n",
    "nEpoch = 1\n",
    "nTrain = label_train.size\n",
    "iIter = 0;\n",
    "for iEpoch in range(nEpoch):\n",
    "    print(iEpoch+1)\n",
    "    random_order = np.random.permutation(nTrain)\n",
    "    for iExample in random_order:\n",
    "        iIter += 1\n",
    "        \n",
    "        # Wake phase (bottom-up pass):\n",
    "        # Drive the network using the example:\n",
    "        network.drive_wake(im_train[iExample, :, :].ravel())\n",
    "        \n",
    "        # Learn the generative weights based on input:\n",
    "        network.train_wake()\n",
    "        \n",
    "        # Sleep phase (top-down pass):\n",
    "        # Drive the network using a dreamed-up (random) fantasy:\n",
    "        network.drive_sleep()\n",
    "        \n",
    "        # Learn the recognition weights based on fantasy:\n",
    "        network.train_sleep()\n",
    "        \n",
    "        # Show training progress by plotting current description length:\n",
    "        if iIter%100 is 0:\n",
    "            xdata.append(iIter)     # Append new data to list\n",
    "            ydata.append(network.get_C())\n",
    "            h_line.set_data(xdata, ydata) # update data\n",
    "            ax.relim()        # Recalculate limits\n",
    "            ax.autoscale_view(True,True,True) #Autoscale\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save layers as list of dicts:\n",
    "netDictList = []\n",
    "for i in range(len(network)):\n",
    "    netDictList.append(network[i].__dict__)\n",
    "    \n",
    "f = open('sleepWake_state_14by14_netIsHintonSized_after10Miter_netDictList.pckl', 'w')\n",
    "pickle.dump(netDictList, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAEKCAYAAACVGgk4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI4QMy/8BZKoJIZlcU4gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a7a400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show some fantasies:\n",
    "%matplotlib inline\n",
    "description_length = 0\n",
    "for i in range(10):\n",
    "    network.drive_sleep()\n",
    "    description_length += network.get_C()\n",
    "    img_row = np.reshape(network.layers[0].state, im_train[0].shape)\n",
    "    for j in range(1, 12):\n",
    "        network.drive_sleep()\n",
    "        description_length += network.get_C()\n",
    "        img = np.reshape(network.layers[0].state, im_train[0].shape)\n",
    "        img_row = np.concatenate((img_row, np.zeros((img.shape[0], 2)), img), 1)\n",
    "    if i is 0:\n",
    "        img_grid = img_row\n",
    "    else:\n",
    "        img_grid = np.concatenate((img_grid, np.zeros((2, img_row.shape[1])), img_row), 0)\n",
    "\n",
    "description_length /= (i+1)*(j+1)\n",
    "plt.imshow(-img_grid, interpolation=\"nearest\", cmap=\"gray\");\n",
    "plt.gca().axis('off');\n",
    "plt.title(('Mean description length: {:1.1f}').format(description_length));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 3 million iterations (rougly the same number of iterations as in the paper), the mean description length is about 2/3 of the \"naive\" description length of the data. The fantasies look reasonably similar to the data, but are a bit noisy.\n",
    "\n",
    "10 more million iterations (several hours of training) improve the mean description length by a few percent and remove some of the noise, but don't make a qualitative difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Back-up network as list of dicts:\n",
    "import pickle\n",
    "netDictList = []\n",
    "for i in range(len(network)):\n",
    "    netDictList.append(network.layers[i].__dict__)\n",
    "    \n",
    "f = open('sleepWake_state_14by14_after10Miter_netDictList.pckl', 'w')\n",
    "pickle.dump(netDictList, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled state:\n",
    "import pickle\n",
    "f = open('sleepWake_state_14by14_after3Miter_netDictList.pckl', 'r')\n",
    "#f = open('sleepWake_state_14by14_after10Miter_netDictList.pckl', 'r')\n",
    "netDictList = pickle.load(f)\n",
    "f.close()\n",
    "for i in range(len(netDictList)):\n",
    "    if i is 0:\n",
    "        network = Network(netDictList[i]['n_units'])\n",
    "    else:\n",
    "        network.addLayer(netDictList[i]['n_units'])\n",
    "        \n",
    "    for key in netDictList[i]:\n",
    "        setattr(network.layers[i], key, netDictList[i][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set process priority low:\n",
    "import win32api,win32process,win32con\n",
    "pid = win32api.GetCurrentProcessId()\n",
    "handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, True, pid)\n",
    "win32process.SetPriorityClass(handle, win32process.BELOW_NORMAL_PRIORITY_CLASS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
